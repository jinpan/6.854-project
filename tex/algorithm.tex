\section{Algorithm}
\subsection{Presentation of algorithm}
Avoiding the theoretical hangups of the most recent publication
improving the bounds for maximum concurrent flow, we choose to
implement the maximum concurrent flow FPAS presented by Garg and
K\"{o}nemann, and several heuristics to speed up runtime in
practice. The basic implementation gives a runtime of
$\~{O}(\omega^-2(k+m)m)$. The dependency on $k$ can be removed with
the implementation of our heuristics as described later. For the
remainder of this section, we will describe the algorithm implemented
in this paper (herein referred to as Garg-MCF), offering both
pseudocode and a brief look at the analysis. \\
The main idea behind Garg-MCF relies on the rerouting and fractional
packing methods described in section 2. The essential ingredients of
these methods include finding an initial infeasible solution, and then
rerouting (typically calling to a known combinatorial optimization
subroutine) based on a length function that grows exponentially in the
amount that is rerouted. Garg-MCF has been theoretically shown to be
much faster and simpler than previous algorithms. This simplicity
makes the algorithm comprehensible. We will be more explicit in how Garg-MCF works
now.

We refer to the second linear program formulation of maximum
concurrent flow, which we will refer to as P-MCF. Taking the dual of
P-MCF generates the following linear program, which defines a length
$l(e)$ for each edge and a variable $z(j)$ for each commodity.
\begin{align*}
\text{min     } \sum_{e\in E}c(e)l(e) \\
\text{s.t. }\sum_{e\in p}l(e) \geq z(j) \;\;\forall 1\leq j \leq k,
\forall p \in \cal{P}_j\\
\sum_{j=1}^kd(j)\cdot z(j)\geq 1\\
l,z\geq 0
\end{align*}
Recalling that $\cal{P}_j$ is the set of paths from $s_j$ to
$t_j$. Intuitively, the first constraint maintains that, when tight, $z(j)$ is the
value of the length of the shortest path from $s_j$ to $t_j$. Keeping
with Garg's notation, we define the objective value to be a function
of the length assignment $l$:
$$D(l) := \sum{e\in E} c(e)l(e)$$
and define $\alpha(l)$ as the second constraint:
$$\alpha(l) := \sum_{j=1}^k d(j)\cdot z(j)$$
Then since we'll have a minimal objective value when the second
constraint is tight, this LP can be viewed as the assignment of
positive lengths to edges such that $\frac{D(l)}{\alpha(l)}$ is
minimized. \\
With notation in hand, the algorithm runs as follows. We first assign
a length of $\delta/c(e)$ to each edge, where $\delta$ is carefully picked
to make the analysis work out. Next we proceed in phases. For each
phase, we have $k$ iterations. For each iteration, we loop through
each commodity $j$ and reroute $d(j)$ units of flow from $s_j$ to
$t_j$. We do this with a series of steps. Let $l_{i,j}^s$ refer to the
length function at the $i^{th}$ phase, the $j^{th}$ iteration and the
$s+1^{th}$ step. Then during each step, we compute the minimum cost
path from $s_j$ to $t_j$ under this length function and route as much
flow along that path as possible. That is, we wish to route a total of
$d(j)$ flow from $s_j$ to $t_j$ over all steps during an iteration, so
let $d_{i,j}^s$ refer to flow remaining to be rerouted during an
iteration, after $s$ steps. Then for each step, we compute a path $p$,
and we route $f_{i,j}^{s+1}$ flow along this path, where
$$f_{i,j}^{s+1}=\min(\min_{e\in p}(c(e)),d_{i,j}^s)$$
We then decrease the amount of flow remaining to be routed by
$f_{i,j}^{s+1}$. Since we routed flow along every edge in $p$, we
need to update the length function for these edges, so we do this by
multiplying $l(e)$ by $1+\epsilon \frac{f_{i,j}^{s+1}}{c(e)}$ for an
$\epsilon$ that we define later, dependent upon $\omega$. We terminate
the iteration when $d_{i,j}^p=0$ for some step number $p$. We repeat
this process for each commodity during an iteration, and we repeat
phases until we reach our stopping condition, which we define when
$D(l_{i})\geq 1$. We then have a graph that has flow along edges, but
is almost surely infeasible. We can scale the flow along each edge by
dividing flow by $\log_{1+\epsilon}\frac{1}{\delta}$, which makes the
flow feasible. From here, we can calculate $\lambda$ directly. 
For clarity, we present the pseudocode of this
algorithm:
\input{pseudo}

\subsection{Analysis}
We offer a brief overview of the analysis. A more complete and
rigorous analysis can be viewed in the original paper. 
\subsubsection{Approximation Ratio}
First we claim
that $\lambda>\frac{t-1}{\log_{1+\epsilon}\frac{1}{\delta}}$ where $t$
is the stopping phase:
For every $c(e)$ units of flow routed through each edge $e$, we
increase the length of $e$ by a factor of $1+\epsilon$. Since the
initial length is $\frac{\delta}{c(e)}$ and $D(t-1)<1$, so
$l_{t-1,k}(e)<\frac{1}{c(e)}$. So we can bound the flow through $e$ in
the first $t-1$ phases is less than
$\log_{1+\epsilon}\frac{1}{\delta}$. If we scale the flow by the scaling
factor described above of $\log_{1+e}\frac{1}{\delta}$ we get a
feasible flow, and have that $$\lambda > \frac{t-1}{\log_{1+\epsilon}\frac{1}{\delta}}$$
Now if we compare the ratio,$\gamma$, of the value of the primal LP $\lambda$ to the value
of the dual solution, $\beta$ using the above bound, we get that the
ratio is strictly less than
$\frac{\beta}{t-1}\log_{1+\epsilon}\delta^{-1}$. \\
Now we seek to bound $\frac{\beta}{t-1}$. We examine
the objective value after the final step of the $j^{th}$ iteration of the $i^{th}$ phase, $D(l_{i,j}^p)$: 
$$D(l_{i,j}^p) = \sum_{e\in E} c(e)l(e) \leq
D(l_{i,j-1}^0)+\epsilon d(j) \mathrm{dist_j}(l_{i,j}^p)$$
where $\mathrm{dist_j}$ defines the length of the shortest path from
$s_j$ to $t_j$ where the argument defines the length function. This
follows since we have to route exactly $d(j)$ units of flow along a
sequence of paths no longer than $\mathrm{dist_j}(l_{i,j}^p)$. We can
then sum over all iterations within a phase to show 
$$D(i)\leq D(i-1)+\epsilon \alpha(i)$$ 
where $D(i)$ refers to the
objective value after the $i^{th}$ phase, and $\alpha(i)$ refers to
$\alpha$ after the $i^{th}$ phase. If we define the minimum value of
$D(i)/\alpha(i)$ to be $\beta$, then we can rewrite this as 
$$D(i)\leq \frac{D(i-1)}{1-\epsilon/\beta}$$. Since we set all lengths
initially to $\delta/c(e)$, $D(0)=m\delta$, so we can show that 
$$D(i)\leq \frac{m\delta}{1-\epsilon} e^{\frac{\epsilon(i-1)}{\beta(1-\epsilon)}}$$
Assuming that $\beta\geq 1$. This assumption will be lifted in section
4.
We stop at the first $t$ such that $D(t)\geq 1$, so we have that 
$$1\leq \frac{m\delta}{1-\epsilon}
e^{\frac{\epsilon(t-1)}{\beta(1-\epsilon)}}$$
which can be rearranged to bound $\frac{\beta}{t-1}$ above by
$\frac{\epsilon}{(1-\epsilon)ln\frac{1-\epsilon}{m\delta}}$.\\
So now we have that
$\gamma<\frac{\beta}{t-1}\log_{1+\epsilon}\delta^{-1}$ we can plug in
the bound for $\frac{beta}{t-1}$. By setting 
$$\delta = (m \cdot (1-\epsilon)^{-1})^{\frac{-1}{\epsilon}})$$
and working through the mathematics, we show that $\gamma \leq
(1-\epsilon)^{-3}$.
Thus we can pick an $\epsilon$ such that $(1-\epsilon)^{-3}$ is no
more than the approximation ratio $1+\omega$. \\
\subsubsection{Runtime}
Finally we argue for runtime. It is enough to bound the number of
steps for each iteration, and the number of phases. We start with
steps.\\
For each step except the last one per iteration, we increase the
length of at least one edge by a factor of $1+\epsilon$. Since each
edge has length between $\delta$ and $1+\epsilon$, we can do no more
than $m\log_{1+\epsilon}\frac{delta}{1+\epsilon}$ steps total, plus
one more step for each iteration. We bound the total number of
iterations as $k$ times the number of phases. The number of phases,
then is $t$ such that 
$$1<\frac{Beta}{t-1}\log_{1+\epsilon}\delta^{-1}$$
where this above inequality comes from our bound on $\gamma$ and weak
duality for LP's, thus the number of phases is less than
$1+\beta\log_{1+\epsilon}\delta^{-1}$ so we have that 
$$t=\lceil \frac{\beta}{\epsilon}\log_{1+\epsilon}
\frac{m}{1-\epsilon} \rceil$$.
Using the $\beta$ scaling trick we will describe in the next section,
we can show that the total number of phases is at most $T\log k$ where 
$$T=\frac{2}{\epsilon} \log_{1+\epsilon}\frac{m}{1-\epsilon}$$.



